# Deep Neural Network Implementation

## General and Framework Papers

* [_Deep Learning with COTS HPC Systems_](http://www.jmlr.org/proceedings/papers/v28/coates13.pdf), A. Coates et al., JMLR 2013
* [_ImageNet Classification with Deep Convolutional Neural Networks_](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf), A. Krizhevsky et al., NIPS 2012
* [_Large Scale Distributed Deep Networks_](http://research.google.com/archive/large_deep_networks_nips2012.html), J. Dean et al, NIPS 2012
* [_Tensorflow: Large-Scale Machine Learing on Heterogeneous Distributed Systems_](https://arxiv.org/abs/1603.04467), M. Abadi et al, arXiv 2016
* [_MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems_](https://arxiv.org/abs/1512.01274), T. Chen et al, arXiv 2016
* [_Deep Image: Scaling up Image Recognition_](https://arxiv.org/vc/arxiv/papers/1501/1501.02876v1.pdf), R. Wu et al, arXiv 2015
* [_Efficient Processing of Deep Neural Networks: A Tutorial and Survey_](https://arxiv.org/abs/1703.09039), V. Sze et al, arXiv 2017
* [_Benchmarking State-of-the-Art Deep Learning Software Tools_](https://arxiv.org/abs/1608.07249), S. Shi et al, arXiv 2017

## Compilation and Optimization Papers

* [_Optimizing Memory Efficiency for Deep Convolutional Neural Networks on GPUs_](https://arxiv.org/pdf/1610.03618v1.pdf), C. Li et al., SC 2016
* [_A Metaprogramming and Autotuning Framework for Deploying Deep Learning Applications_](https://arxiv.org/abs/1611.06945), M. Moskewicz et al., arXiv 2016
* [_One Weird Trick for Parallelizing Convolutional Neural Networks_](https://arxiv.org/abs/1404.5997), A. Krizhevsky, arXiv 2015
* [_Persistent RNNs: Stashing Recurrent Weights On-Chip_](http://jmlr.org/proceedings/papers/v48/diamos16.pdf), G. Diamos et al, ICML 2016
* [_Latte: A Language, Compiler, and Runtime for Elegant and Efficient Deep Neural Networks_](http://www.thev.net/PaulLiu/download/p209-truong.pdf), L. Truong et al., PLDI 2016
* [_On Optimizing Machine Learning Workloads via Kernel Fusion_](http://dl.acm.org/citation.cfm?id=2688521), A. Ashari et al., PPoPP 2015
* [_Memory-Efficient Backpropagation Through Time_](https://arxiv.org/abs/1606.03401), A. Gruslys et al, arXiv 2016
* [_Training Deep Nets with Sublinear Memory Cost_](https://arxiv.org/abs/1604.06174), T. Chen et al, arXiv 2016

## Dynamic Neural Networks

* [_Deep Learning with Dynamic Computation Graphs_](https://arxiv.org/abs/1702.02181), M. Looks et al, ICLR 2017
* [_DyNet: The Dynamic Neural Network Toolkit_](https://arxiv.org/abs/1701.03980), G. Neubig et al, arXiv 2017

## Online Articles and Blog Posts

* [_How to Parallelize Deep Learning on GPUs Part 1_](http://timdettmers.com/2014/10/09/deep-learning-data-parallelism/)
* [_How to Parallelize Deep Learning on GPUs Part 2_](http://timdettmers.com/2014/11/09/model-parallelism-deep-learning/)
* [_Scaling Deep Learning with MXNet_](https://www.slideshare.net/AIFrontiers/scaling-deep-learning-with-mxnet)
* [_Rolling and Unrolling RNNs_](https://shapeofdata.wordpress.com/2016/04/27/rolling-and-unrolling-rnns/)
* [_Optimizing Memory Consumption in Deep Learning_](http://mxnet.io/architecture/note_memory.html)

## Neural Network Optimization Frameworks

* [_TensorFlow XLA_](https://www.tensorflow.org/performance/xla/)
* [_NNVM: Neural Network Virtual Machine_](https://github.com/dmlc/nnvm)
* [_Nervana Systems ngraph_](https://github.com/NervanaSystems/ngraph)
* [_Boda Compute Graph_](https://github.com/moskewcz/boda)
* [_DyNet_](https://github.com/clab/dynet)
* [_OptNet_](https://github.com/fmassa/optimize-net)
* [_TensorFlow Graph Transform Tool_](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md)
* 
